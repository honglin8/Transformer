{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `SelfAttention`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size // heads\n",
    "\n",
    "        assert (\n",
    "            self.head_dim * self.heads == self.embed_size\n",
    "        ), \"Embedding size needs to be divisible by heads\"\n",
    "\n",
    "        self.values = nn.Linear(embed_size, embed_size)\n",
    "        self.keys = nn.Linear(embed_size, embed_size)\n",
    "        self.queries = nn.Linear(embed_size, embed_size)\n",
    "        self.fc_out = nn.Linear(embed_size, embed_size)\n",
    "\n",
    "    def forward(self, values, keys, query, mask):\n",
    "        N = query.shape[0] # Batch size\n",
    "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
    "        # Shapes before splitting:\n",
    "        # values:  (N, value_len, embed_size)\n",
    "        # keys:    (N, key_len  , embed_size)\n",
    "        # queries: (N, query_len, embed_size)\n",
    "\n",
    "        # Split the embedding into self.heads different pieces\n",
    "        values = self.values(values).reshape(N, value_len, self.heads, self.head_dim)\n",
    "        keys = self.keys(keys).reshape(N, key_len, self.heads, self.head_dim)\n",
    "        queries = self.queries(query).reshape(N, query_len, self.heads, self.head_dim)\n",
    "        # Shapes after splitting: \n",
    "        # values:  (N, value_len, heads, head_dim)\n",
    "        # keys:    (N, key_len  , heads, head_dim)\n",
    "        # queries: (N, query_len, heads, head_dim)\n",
    "\n",
    "        # Compute the dot product between queries and keys for each head, \n",
    "        # and divide by sqrt of head_dim for numerical stability\n",
    "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys]) / math.sqrt(self.head_dim)\n",
    "        # Shape of energy: (N, heads, query_len, key_len)\n",
    "\n",
    "        # Apply mask\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "        # Shape of mask: (N, 1, 1, key_len)\n",
    "        # 0 in key_len dimension means that the respective element in energy is set to -1e20\n",
    "        # Mask will be broadcasted to (N, heads, query_len, key_len) by PyTorch automatically\n",
    "\n",
    "        # Compute the attention weights for each head using the softmax function\n",
    "        attention = torch.softmax(energy, dim=-1)\n",
    "        # Shape of attention: (N, heads, query_len, key_len)\n",
    "\n",
    "        # Multiply the attention weights with the values for each head and then concatenate\n",
    "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
    "            N, query_len, self.embed_size\n",
    "        )\n",
    "        # Shape of out: (N, query_len, embed_size)\n",
    "\n",
    "        out = self.fc_out(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `TransformerBlock`: \n",
    "\n",
    "`SelfAttention` -> layerNorm -> Feed-Forward -> layerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, forward_expansion, dropout):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = SelfAttention(embed_size, heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_size, forward_expansion * embed_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(forward_expansion * embed_size, embed_size),\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, value, key, query, mask):\n",
    "        # Compute self-attention\n",
    "        attention = self.attention(value, key, query, mask)\n",
    "        # Shape of attention: (N, query_len, embed_size)\n",
    "\n",
    "        # Add skip connection, run through normalization and dropout\n",
    "        x = self.dropout(self.norm1(attention + query))\n",
    "        # Shape of x: (N, query_len, embed_size)\n",
    "\n",
    "        # Feed-forward network\n",
    "        forward = self.feed_forward(x)\n",
    "        # Shape of forward: (N, query_len, embed_size)\n",
    "\n",
    "        # Add skip connection, run through normalization and dropout\n",
    "        out = self.dropout(self.norm2(forward + x))\n",
    "        # Shape of out: (N, query_len, embed_size)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder: `num_layers` of `TransformerBlock` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        src_vocab_size, # Size of the source vocabulary\n",
    "        num_layers, # Number of TransformerBlocks\n",
    "        max_length, # Maximum length of the sentence\n",
    "        embed_size,\n",
    "        heads,\n",
    "        forward_expansion,\n",
    "        dropout,\n",
    "        device,\n",
    "    ):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.device = device\n",
    "        self.word_embedding = nn.Embedding(src_vocab_size, embed_size)\n",
    "        self.position_embedding = nn.Embedding(max_length, embed_size)\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                TransformerBlock(\n",
    "                    embed_size,\n",
    "                    heads,\n",
    "                    forward_expansion,\n",
    "                    dropout,\n",
    "                )\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        N, seq_length = x.shape\n",
    "\n",
    "        # Positions is the index of the word in the sentence (0, 1, 2, ..., seq_length)\n",
    "        positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)\n",
    "\n",
    "        # Add word embeddings and position embeddings\n",
    "        out = self.dropout(\n",
    "            (self.word_embedding(x) + self.position_embedding(positions))\n",
    "        )\n",
    "        # Shape of out: (N, seq_length, embed_size)\n",
    "\n",
    "        # In the Encoder the query, key, value are all the same\n",
    "        for layer in self.layers:\n",
    "            out = layer(out, out, out, mask)\n",
    "            # Shape of out: (N, seq_length, embed_size)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `DecoderBlock`\n",
    "\n",
    "Masked `SelfAttention` -> layerNorm -> `TransformerBlock`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, forward_expansion, dropout):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.norm = nn.LayerNorm(embed_size)\n",
    "        self.masked_attention = SelfAttention(embed_size, heads=heads)\n",
    "        self.transformer_block = TransformerBlock(\n",
    "            embed_size, heads, forward_expansion, dropout\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, encoder_value, encoder_key, src_mask, trg_mask):\n",
    "        # Self attention on the target sentence with mask\n",
    "        attention = self.masked_attention(x, x, x, trg_mask)\n",
    "\n",
    "        # Add skip connection, run through normalization and dropout\n",
    "        query = self.dropout(self.norm(attention + x))\n",
    "\n",
    "        # Transformer block with encoder's output as value and key\n",
    "        out = self.transformer_block(encoder_value, encoder_key, query, src_mask)\n",
    "        # Shape of out: (N, query_len, embed_size)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder: `num_layers` of `DecoderBlock`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        trg_vocab_size, # Size of the target vocabulary\n",
    "        num_layers, # Number of DecoderBlocks\n",
    "        max_length, # Maximum length of the sentence\n",
    "        embed_size,\n",
    "        heads,\n",
    "        forward_expansion,\n",
    "        dropout,\n",
    "        device,\n",
    "    ):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.device = device\n",
    "        self.word_embedding = nn.Embedding(trg_vocab_size, embed_size)\n",
    "        self.position_embedding = nn.Embedding(max_length, embed_size)\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                DecoderBlock(\n",
    "                    embed_size,\n",
    "                    heads,\n",
    "                    forward_expansion,\n",
    "                    dropout)\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.fc_out = nn.Linear(embed_size, trg_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, enc_out, src_mask, trg_mask):\n",
    "        N, seq_length = x.shape\n",
    "\n",
    "        # Positions is the index of the word in the sentence (0, 1, 2, ..., seq_length)\n",
    "        positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)\n",
    "\n",
    "        # Add word embeddings and position embeddings\n",
    "        x = self.dropout((self.word_embedding(x) + self.position_embedding(positions)))\n",
    "\n",
    "        # In the Decoder the key and value are the encoder's output,\n",
    "        # and the query is the output of the previous DecoderBlock\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, enc_out, enc_out, src_mask, trg_mask)\n",
    "\n",
    "        out = self.fc_out(x)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Transformer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        src_vocab_size,\n",
    "        trg_vocab_size,\n",
    "        src_pad_idx, # Index of the padding token in the source vocabulary\n",
    "        trg_pad_idx, # Index of the padding token in the target vocabulary\n",
    "        num_layers,\n",
    "        max_length,\n",
    "        embed_size,\n",
    "        heads,\n",
    "        forward_expansion=4,\n",
    "        dropout=0.0,\n",
    "        device=\"cpu\",\n",
    "    ):\n",
    "\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        # Initialize the Encoder\n",
    "        self.encoder = Encoder(\n",
    "            src_vocab_size,\n",
    "            num_layers,\n",
    "            max_length,\n",
    "            embed_size,\n",
    "            heads,\n",
    "            forward_expansion,\n",
    "            dropout,\n",
    "            device,\n",
    "        )\n",
    "\n",
    "        # Initialize the Decoder\n",
    "        self.decoder = Decoder(\n",
    "            trg_vocab_size,\n",
    "            num_layers,\n",
    "            max_length,\n",
    "            embed_size,\n",
    "            heads,\n",
    "            forward_expansion,\n",
    "            dropout,\n",
    "            device,\n",
    "        )\n",
    "\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.device = device\n",
    "\n",
    "    def make_src_mask(self, src):\n",
    "        # Shape of src: (N, src_len)\n",
    "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        # Shape of src_mask: (N, 1, 1, src_len)\n",
    "        return src_mask.to(self.device)\n",
    "\n",
    "    def make_trg_mask(self, trg):\n",
    "        N, trg_len = trg.shape\n",
    "\n",
    "        # Create a lower triangular matrix of ones with shape (trg_len, trg_len),\n",
    "        # then expand it to (N, 1, trg_len, trg_len)\n",
    "        trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(\n",
    "            N, 1, trg_len, trg_len\n",
    "        )\n",
    "\n",
    "        return trg_mask.to(self.device)\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        src_mask = self.make_src_mask(src)\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "        enc_src = self.encoder(src, src_mask)\n",
    "        out = self.decoder(trg, enc_src, src_mask, trg_mask) # raw l\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Transformer` Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Set seed for reproducibility\n",
    "seed = 4212\n",
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sos_idx = 100 # Start of sequence index\n",
    "eos_idx = 101 # End of sequence index\n",
    "pad_idx = 0 # Padding index\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Hyperparameters\n",
    "num_samples = 5000 # Number of samples in the dataset\n",
    "max_length = 6 + 2 # Maximum length of the sequence including the <SOS> and <EOS> tokens\n",
    "vocab_size = 99 + 3 # Numbers from 1 to 99 and three indices: padding, sos, eos\n",
    "num_layers = 2 # Number of Blocks in the Encoder and Decoder\n",
    "embed_size = 32\n",
    "heads = 2\n",
    "forward_expansion = 4\n",
    "dropout = 0.0\n",
    "learning_rate = 0.001\n",
    "batch_size = 256\n",
    "num_epochs = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate data for the three tasks: copy, reverse, and sort\n",
    "def generate_data(num_samples, max_length, vocab_size, pad_idx, sos_idx, eos_idx, task, seed=None):\n",
    "    if seed is not None:\n",
    "        set_seed(seed)\n",
    "\n",
    "    src_data = []\n",
    "    trg_data = []\n",
    "    \n",
    "    for _ in range(num_samples):\n",
    "        seq_length = np.random.randint(1, max_length - 1)  # Length of the random sequence\n",
    "        sequence = np.random.randint(1, vocab_size - 2, seq_length).tolist()\n",
    "        \n",
    "        if task == 'copy':\n",
    "            target_sequence = sequence\n",
    "        elif task == 'sort':\n",
    "            target_sequence = sorted(sequence)\n",
    "        elif task == 'reverse':\n",
    "            target_sequence = sequence[::-1]\n",
    "        else:\n",
    "            raise ValueError(\"Unknown task\")\n",
    "        \n",
    "        # Add the <SOS> and <EOS> tokens, then pad to the maximum length\n",
    "        src_sequence = [sos_idx] + sequence + [eos_idx] + [pad_idx] * (max_length - len(sequence) - 2)\n",
    "        trg_sequence = [sos_idx] + target_sequence + [eos_idx] + [pad_idx] * (max_length - len(target_sequence) - 2)\n",
    "        \n",
    "        src_data.append(src_sequence)\n",
    "        trg_data.append(trg_sequence)\n",
    "    \n",
    "    src_data = torch.tensor(src_data, dtype=torch.long)\n",
    "    trg_data = torch.tensor(trg_data, dtype=torch.long)\n",
    "    \n",
    "    return src_data, trg_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copy Task - Source Data:\n",
      "tensor([[100,  10,  14,  82, 101,   0,   0,   0],\n",
      "        [100,  99,  36,  70,  98, 101,   0,   0],\n",
      "        [100,  61, 101,   0,   0,   0,   0,   0]])\n",
      "Copy Task - Target Data:\n",
      "tensor([[100,  10,  14,  82, 101,   0,   0,   0],\n",
      "        [100,  99,  36,  70,  98, 101,   0,   0],\n",
      "        [100,  61, 101,   0,   0,   0,   0,   0]])\n",
      "\n",
      "Reverse Task - Source Data:\n",
      "tensor([[100,  31, 101,   0,   0,   0,   0,   0],\n",
      "        [100,   7,  38,  70, 101,   0,   0,   0],\n",
      "        [100,  84,  72,  97,   2,   1,  99, 101]])\n",
      "Reverse Task - Target Data:\n",
      "tensor([[100,  31, 101,   0,   0,   0,   0,   0],\n",
      "        [100,  70,  38,   7, 101,   0,   0,   0],\n",
      "        [100,  99,   1,   2,  97,  72,  84, 101]])\n",
      "\n",
      "Sort Task - Source Data:\n",
      "tensor([[100,  58,  15, 101,   0,   0,   0,   0],\n",
      "        [100,  83, 101,   0,   0,   0,   0,   0],\n",
      "        [100,  75,  71,  24, 101,   0,   0,   0]])\n",
      "Sort Task - Target Data:\n",
      "tensor([[100,  15,  58, 101,   0,   0,   0,   0],\n",
      "        [100,  83, 101,   0,   0,   0,   0,   0],\n",
      "        [100,  24,  71,  75, 101,   0,   0,   0]])\n"
     ]
    }
   ],
   "source": [
    "set_seed(seed)\n",
    "\n",
    "# Generate data for the copying task\n",
    "src_data_copy, trg_data_copy = generate_data(num_samples, max_length, vocab_size, pad_idx, sos_idx, eos_idx, 'copy',)\n",
    "\n",
    "# Generate data for the reversing task\n",
    "src_data_reverse, trg_data_reverse = generate_data(num_samples, max_length, vocab_size, pad_idx, sos_idx, eos_idx, 'reverse')\n",
    "\n",
    "# Generate data for the sorting task\n",
    "src_data_sort, trg_data_sort = generate_data(num_samples, max_length, vocab_size, pad_idx, sos_idx, eos_idx, 'sort')\n",
    "\n",
    "print(\"Copy Task - Source Data:\")\n",
    "print(src_data_copy[:3])\n",
    "print(\"Copy Task - Target Data:\")\n",
    "print(trg_data_copy[:3])\n",
    "print()\n",
    "print(\"Reverse Task - Source Data:\")\n",
    "print(src_data_reverse[:3])\n",
    "print(\"Reverse Task - Target Data:\")\n",
    "print(trg_data_reverse[:3])\n",
    "print()\n",
    "print(\"Sort Task - Source Data:\")\n",
    "print(src_data_sort[:3])\n",
    "print(\"Sort Task - Target Data:\")\n",
    "print(trg_data_sort[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "def prepare_data(src_data, trg_data, test_size=0.2, seed=None):\n",
    "    if seed is not None:\n",
    "        set_seed(seed)\n",
    "\n",
    "    src_train, src_test, trg_train, trg_test = train_test_split(src_data, trg_data, test_size=test_size)\n",
    "    train_dataset = TensorDataset(src_train, trg_train)\n",
    "    test_dataset = TensorDataset(src_test, trg_test)\n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the training dataset: 4000\n",
      "Size of the test dataset: 1000\n"
     ]
    }
   ],
   "source": [
    "set_seed(4212)\n",
    "\n",
    "# For the 'copy' task\n",
    "train_data, test_data = prepare_data(src_data_copy, trg_data_copy)\n",
    "train_loader_copy = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader_copy = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "# For the 'reverse' task\n",
    "train_data, test_data = prepare_data(src_data_reverse, trg_data_reverse)\n",
    "train_loader_reverse = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader_reverse = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "# For the 'sort' task\n",
    "train_data, test_data = prepare_data(src_data_sort, trg_data_sort)\n",
    "train_loader_sort = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader_sort = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "print(f'Size of the training dataset: {len(train_data)}')\n",
    "print(f'Size of the test dataset: {len(test_data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train the model\n",
    "def train_model(model, train_loader, criterion, optimizer, num_epochs, device):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for src, trg in train_loader:\n",
    "            src = src.to(device)\n",
    "            trg = trg.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            output = model(src, trg[:, :-1]) # Exclude the last token from the target sequence\n",
    "            output = output.reshape(-1, output.shape[2])\n",
    "            trg = trg[:, 1:].reshape(-1) # Exclude the <SOS> token from the target sequence\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        if epoch % 2 == 1:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss/len(train_loader)}')\n",
    "\n",
    "    print(\"Training complete.\")\n",
    "    return model\n",
    "\n",
    "# Define the loss function\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/20], Loss: 4.102633774280548\n",
      "Epoch [4/20], Loss: 3.926971048116684\n",
      "Epoch [6/20], Loss: 3.3467292189598083\n",
      "Epoch [8/20], Loss: 2.0810547322034836\n",
      "Epoch [10/20], Loss: 1.0563989281654358\n",
      "Epoch [12/20], Loss: 0.48228208534419537\n",
      "Epoch [14/20], Loss: 0.23481028713285923\n",
      "Epoch [16/20], Loss: 0.13550485204905272\n",
      "Epoch [18/20], Loss: 0.08979252399876714\n",
      "Epoch [20/20], Loss: 0.06490117637440562\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "set_seed(4212)\n",
    "\n",
    "# Initialize the Transformer model for the 'copy' task\n",
    "model_copy = Transformer(\n",
    "    src_vocab_size=vocab_size,\n",
    "    trg_vocab_size=vocab_size,\n",
    "    src_pad_idx=pad_idx,\n",
    "    trg_pad_idx=pad_idx,\n",
    "    num_layers=num_layers,\n",
    "    max_length=max_length,\n",
    "    embed_size=embed_size,\n",
    "    heads=heads,\n",
    "    forward_expansion=forward_expansion,\n",
    "    dropout=dropout,\n",
    "    device=device\n",
    ").to(device)\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer_copy = optim.Adam(model_copy.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model for the 'copy' task\n",
    "model_copy = train_model(model_copy, train_loader_copy, criterion, optimizer_copy, num_epochs, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reverse Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/20], Loss: 4.10052752494812\n",
      "Epoch [4/20], Loss: 3.9241564571857452\n",
      "Epoch [6/20], Loss: 3.54915614426136\n",
      "Epoch [8/20], Loss: 3.142897590994835\n",
      "Epoch [10/20], Loss: 2.496100053191185\n",
      "Epoch [12/20], Loss: 1.5218461453914642\n",
      "Epoch [14/20], Loss: 0.6681469455361366\n",
      "Epoch [16/20], Loss: 0.28762717358767986\n",
      "Epoch [18/20], Loss: 0.155604082159698\n",
      "Epoch [20/20], Loss: 0.09971246775239706\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "set_seed(4212)\n",
    "\n",
    "# Initialize the Transformer model for the 'reverse' task\n",
    "model_reverse = Transformer(\n",
    "    src_vocab_size=vocab_size,\n",
    "    trg_vocab_size=vocab_size,\n",
    "    src_pad_idx=pad_idx,\n",
    "    trg_pad_idx=pad_idx,\n",
    "    num_layers=num_layers,\n",
    "    max_length=max_length,\n",
    "    embed_size=embed_size,\n",
    "    heads=heads,\n",
    "    forward_expansion=forward_expansion,\n",
    "    dropout=dropout,\n",
    "    device=device\n",
    ").to(device)\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer_reverse = optim.Adam(model_reverse.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model for the 'reverse' task\n",
    "model_reverse = train_model(model_reverse, train_loader_reverse, criterion, optimizer_reverse, num_epochs, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/20], Loss: 4.047736406326294\n",
      "Epoch [4/20], Loss: 3.688209608197212\n",
      "Epoch [6/20], Loss: 3.2033394277095795\n",
      "Epoch [8/20], Loss: 2.660450294613838\n",
      "Epoch [10/20], Loss: 2.0859378278255463\n",
      "Epoch [12/20], Loss: 1.5131881088018417\n",
      "Epoch [14/20], Loss: 0.9860993400216103\n",
      "Epoch [16/20], Loss: 0.6012790687382221\n",
      "Epoch [18/20], Loss: 0.39554205164313316\n",
      "Epoch [20/20], Loss: 0.2926633283495903\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "set_seed(4212)\n",
    "\n",
    "# Initialize the Transformer model for the 'sort' task\n",
    "model_sort = Transformer(\n",
    "    src_vocab_size=vocab_size,\n",
    "    trg_vocab_size=vocab_size,\n",
    "    src_pad_idx=pad_idx,\n",
    "    trg_pad_idx=pad_idx,\n",
    "    num_layers=num_layers,\n",
    "    max_length=max_length,\n",
    "    embed_size=embed_size,\n",
    "    heads=heads,\n",
    "    forward_expansion=forward_expansion,\n",
    "    dropout=dropout,\n",
    "    device=device\n",
    ").to(device)\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer_sort = optim.Adam(model_sort.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model for the 'sort' task\n",
    "model_sort = train_model(model_sort, train_loader_sort, criterion, optimizer_sort, num_epochs, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate output for a given batch of input tensors\n",
    "def generate_output(model, input_tensors, max_length, pad_idx, sos_idx, eos_idx, device):\n",
    "    model.eval()\n",
    "    batch_size = input_tensors.size(0)\n",
    "    with torch.no_grad():\n",
    "        trg_tensors = torch.full((batch_size, max_length), pad_idx).to(device)\n",
    "        trg_tensors[:, 0] = sos_idx\n",
    "        \n",
    "        for i in range(1, max_length):\n",
    "            output = model(input_tensors, trg_tensors[:, :i])\n",
    "            next_tokens = output.argmax(2)[:, -1]\n",
    "            trg_tensors[:, i] = next_tokens\n",
    "            \n",
    "            # Check if EOS token is predicted for all sequences in the batch\n",
    "            if (next_tokens == eos_idx).all():\n",
    "                break\n",
    "        \n",
    "        # Fill the rest with padding tokens after EOS token\n",
    "        for j in range(batch_size):\n",
    "            eos_position = (trg_tensors[j] == eos_idx).nonzero(as_tuple=True)[0]\n",
    "            if eos_position.numel() > 0:\n",
    "                eos_position = eos_position[0].item()\n",
    "                trg_tensors[j, eos_position + 1:] = pad_idx\n",
    "        \n",
    "        return trg_tensors\n",
    "    \n",
    "# Function to process the predicted output tensor and return the predicted tokens    \n",
    "def process_output(output_tensor, pad_idx, sos_idx, eos_idx):\n",
    "    output_tokens = output_tensor.squeeze().tolist()\n",
    "    processed_tokens = [token for token in output_tokens if token not in [pad_idx, sos_idx, eos_idx]]\n",
    "    return processed_tokens\n",
    "\n",
    "# Function to evaluate the model on the test set; returns the average loss and sequence-level accuracy\n",
    "def evaluate_model(model, test_loader, criterion, max_length, pad_idx, sos_idx, eos_idx, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_correct_sequences = 0\n",
    "    total_sequences = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for src, trg in test_loader:\n",
    "            src = src.to(device)\n",
    "            trg = trg.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            output = model(src, trg[:, :-1])\n",
    "            \n",
    "            # Reshape output and target to match the expected input shape for CrossEntropyLoss\n",
    "            output = output.reshape(-1, output.shape[2])\n",
    "            trg_flat = trg[:, 1:].reshape(-1)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(output, trg_flat)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Generate predictions\n",
    "            output_tensors = generate_output(model, src, max_length, pad_idx, sos_idx, eos_idx, device)\n",
    "            \n",
    "            # Compare the tensors directly for sequence-level accuracy\n",
    "            for i in range(src.size(0)):\n",
    "                target_tensor = trg[i].unsqueeze(0)\n",
    "                predicted_tensor = output_tensors[i].unsqueeze(0)\n",
    "                \n",
    "                if torch.equal(target_tensor, predicted_tensor):\n",
    "                    total_correct_sequences += 1\n",
    "\n",
    "                total_sequences += 1\n",
    "    \n",
    "    average_loss = total_loss / len(test_loader)\n",
    "    accuracy = total_correct_sequences / total_sequences\n",
    "    return average_loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copy Task:\n",
      "Average Loss: 0.0611, Accuracy: 1.0000\n",
      "\n",
      "Reverse Task:\n",
      "Average Loss: 0.0949, Accuracy: 0.9970\n",
      "\n",
      "Sort Task:\n",
      "Average Loss: 0.3399, Accuracy: 0.7550\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the models on their respective test sets\n",
    "average_loss_copy, accuracy_copy = evaluate_model(model_copy, test_loader_copy, criterion, max_length, pad_idx, sos_idx, eos_idx, device)\n",
    "average_loss_reverse, accuracy_reverse = evaluate_model(model_reverse, test_loader_reverse, criterion, max_length, pad_idx, sos_idx, eos_idx, device)\n",
    "average_loss_sort, accuracy_sort = evaluate_model(model_sort, test_loader_sort, criterion, max_length, pad_idx, sos_idx, eos_idx, device)\n",
    "\n",
    "print(\"Copy Task:\")\n",
    "print(f\"Average Loss: {average_loss_copy:.4f}, Accuracy: {accuracy_copy:.4f}\")\n",
    "print()\n",
    "print(\"Reverse Task:\")\n",
    "print(f\"Average Loss: {average_loss_reverse:.4f}, Accuracy: {accuracy_reverse:.4f}\")\n",
    "print()\n",
    "print(\"Sort Task:\")\n",
    "print(f\"Average Loss: {average_loss_sort:.4f}, Accuracy: {accuracy_sort:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A Single Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sequence: [6, 5, 1, 4, 3]\n",
      "\n",
      "Input tensor: tensor([[100,   6,   5,   1,   4,   3, 101,   0]])\n",
      "Output tensor (Copy):    tensor([[100,   6,   5,   1,   4,   3, 101,   0]])\n",
      "Output tensor (Reverse): tensor([[100,   3,   4,   1,   5,   6, 101,   0]])\n",
      "Output tensor (Sort):    tensor([[100,   1,   3,   5,   4, 101,   0,   0]])\n",
      "\n",
      "Task: Copy\n",
      "Predicted Output: [6, 5, 1, 4, 3]\n",
      "Expected Output:  [6, 5, 1, 4, 3]\n",
      "Match?: True\n",
      "\n",
      "Task: Reverse\n",
      "Predicted Output: [3, 4, 1, 5, 6]\n",
      "Expected Output:  [3, 4, 1, 5, 6]\n",
      "Match?: True\n",
      "\n",
      "Task: Sort\n",
      "Predicted Output: [1, 3, 5, 4]\n",
      "Expected Output:  [1, 3, 4, 5, 6]\n",
      "Match?: False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example input sequence (maximum length: 6)\n",
    "input_sequence = [6,5,1,4,3]\n",
    "\n",
    "# Prepare the input sequence: Add <SOS> and <EOS> tokens, pad the sequence to the maximum length\n",
    "input_tensor = torch.tensor([sos_idx] + input_sequence + [eos_idx] + [pad_idx] * (max_length - len(input_sequence) - 2)).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "# Move the input tensor to the appropriate device\n",
    "input_tensor = input_tensor.to(device)\n",
    "\n",
    "# Generate output for each model\n",
    "output_tensor_copy = generate_output(model_copy, input_tensor, max_length, pad_idx, sos_idx, eos_idx, device)\n",
    "output_tensor_reverse = generate_output(model_reverse, input_tensor, max_length, pad_idx, sos_idx, eos_idx, device)\n",
    "output_tensor_sort = generate_output(model_sort, input_tensor, max_length, pad_idx, sos_idx, eos_idx, device)\n",
    "\n",
    "# Remove padding, start, and end tokens\n",
    "predicted_output_copy = process_output(output_tensor_copy, pad_idx, sos_idx, eos_idx)\n",
    "predicted_output_reverse = process_output(output_tensor_reverse, pad_idx, sos_idx, eos_idx)\n",
    "predicted_output_sort = process_output(output_tensor_sort, pad_idx, sos_idx, eos_idx)\n",
    "\n",
    "# Expected outputs for each task\n",
    "expected_output_copy = input_sequence\n",
    "expected_output_reverse = input_sequence[::-1]\n",
    "expected_output_sort = sorted(input_sequence)\n",
    "\n",
    "print(\"Input Sequence:\", input_sequence)\n",
    "print()\n",
    "print(f'Input tensor: {input_tensor}')\n",
    "print(f'Output tensor (Copy):    {output_tensor_copy}')\n",
    "print(f'Output tensor (Reverse): {output_tensor_reverse}')\n",
    "print(f'Output tensor (Sort):    {output_tensor_sort}')\n",
    "print()\n",
    "\n",
    "print(f'Task: Copy')\n",
    "print(f'Predicted Output: {predicted_output_copy}')\n",
    "print(f'Expected Output:  {expected_output_copy}')\n",
    "print(f'Match?: {predicted_output_copy == expected_output_copy}')\n",
    "print()\n",
    "\n",
    "print(f'Task: Reverse')\n",
    "print(f'Predicted Output: {predicted_output_reverse}')\n",
    "print(f'Expected Output:  {expected_output_reverse}')\n",
    "print(f'Match?: {predicted_output_reverse == expected_output_reverse}')\n",
    "print()\n",
    "\n",
    "print(f'Task: Sort')\n",
    "print(f'Predicted Output: {predicted_output_sort}')\n",
    "print(f'Expected Output:  {expected_output_sort}')\n",
    "print(f'Match?: {predicted_output_sort == expected_output_sort}')\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
