{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from jax import random, grad, jit\n",
    "from jax.nn import softmax, relu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Embeddings and Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(vocab_size, embed_dim):\n",
    "    return random.normal(random.PRNGKey(0), (vocab_size, embed_dim))\n",
    "\n",
    "def get_positional_encoding(seq_len, embed_dim):\n",
    "    pe = np.zeros((seq_len, embed_dim))\n",
    "    for pos in range(seq_len):\n",
    "        for i in range(0, embed_dim, 2):\n",
    "            pe[pos, i] = np.sin(pos / (10000 ** (2 * i / embed_dim)))\n",
    "            pe[pos, i + 1] = np.cos(pos / (10000 ** (2 * i / embed_dim)))\n",
    "    return np.array(pe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(Q, K, V):\n",
    "    d_k = Q.shape[1]\n",
    "    scores = jnp.dot(Q, K.T) / jnp.sqrt(d_k)\n",
    "    weights = softmax(scores, axis=-1)\n",
    "    return jnp.dot(weights, V)\n",
    "\n",
    "def multi_head_attention(Q, K, V, num_heads, key):\n",
    "    d_model = Q.shape[1]\n",
    "    assert d_model % num_heads == 0\n",
    "    depth = d_model // num_heads\n",
    "    keys = random.split(key, num_heads)\n",
    "    \n",
    "    Q_split = jnp.concatenate([Q @ random.normal(keys[i], (d_model, depth)) for i in range(num_heads)], axis=0)\n",
    "    K_split = jnp.concatenate([K @ random.normal(keys[i], (d_model, depth)) for i in range(num_heads)], axis=0)\n",
    "    V_split = jnp.concatenate([V @ random.normal(keys[i], (d_model, depth)) for i in range(num_heads)], axis=0)\n",
    "    \n",
    "    attention_heads = [scaled_dot_product_attention(Q_split[i], K_split[i], V_split[i]) for i in range(num_heads)]\n",
    "    concat_attention = jnp.concatenate(attention_heads, axis=-1)\n",
    "    \n",
    "    W_O = random.normal(key, (d_model, d_model))\n",
    "    return concat_attention @ W_O"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder and Decoder Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_layer(X, num_heads, key):\n",
    "    attn_output = multi_head_attention(X, X, X, num_heads, key)\n",
    "    attn_output += X  # Add residual connection\n",
    "    ff_output = relu(attn_output @ random.normal(key, (attn_output.shape[-1], attn_output.shape[-1])))\n",
    "    return ff_output\n",
    "\n",
    "def decoder_layer(Y, encoder_output, num_heads, key):\n",
    "    self_attn_output = multi_head_attention(Y, Y, Y, num_heads, key)\n",
    "    cross_attn_output = multi_head_attention(self_attn_output, encoder_output, encoder_output, num_heads, key)\n",
    "    cross_attn_output += Y  # Add residual connection\n",
    "    ff_output = relu(cross_attn_output @ random.normal(key, (cross_attn_output.shape[-1], cross_attn_output.shape[-1])))\n",
    "    return ff_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_model(X, Y, vocab_size, embed_dim, seq_len_encoder, seq_len_decoder, num_heads, num_layers, key):\n",
    "    encoder_embeddings = get_embeddings(vocab_size, embed_dim, key)\n",
    "    decoder_embeddings = get_embeddings(vocab_size, embed_dim, key)\n",
    "    encoder_positional_encoding = get_positional_encoding(seq_len_encoder, embed_dim)\n",
    "    decoder_positional_encoding = get_positional_encoding(seq_len_decoder, embed_dim)\n",
    "\n",
    "    # Apply embeddings and positional encodings to encoder and decoder inputs\n",
    "    X_embed = encoder_embeddings[X] + encoder_positional_encoding  # Shape: (1, 7, 16)\n",
    "    Y_embed = decoder_embeddings[Y] + decoder_positional_encoding  # Shape: (1, 4, 16)\n",
    "\n",
    "    # Encoder pass\n",
    "    encoder_output = X_embed\n",
    "    for _ in range(num_layers):\n",
    "        encoder_output = encoder_layer(encoder_output, num_heads, key)\n",
    "\n",
    "    # Decoder pass\n",
    "    decoder_output = Y_embed\n",
    "    for _ in range(num_layers):\n",
    "        decoder_output = decoder_layer(decoder_output, encoder_output, num_heads, key)\n",
    "    \n",
    "    # Final output layer (logits)\n",
    "    logits = softmax(decoder_output @ encoder_embeddings.T)  # Shape: (1, 4, vocab_size)\n",
    "    return logits\n",
    "\n",
    "def loss_fn(logits, labels):\n",
    "    return -jnp.mean(jnp.sum(labels * jnp.log(logits), axis=-1))\n",
    "\n",
    "key = random.PRNGKey(0)\n",
    "embed_dim = 16\n",
    "seq_len_encoder = X_train.shape[1]\n",
    "seq_len_decoder = Y_train.shape[1]\n",
    "vocab_size = max(token_map.values()) + 1\n",
    "num_heads = 2\n",
    "num_layers = 2\n",
    "\n",
    "for epoch in range(1000):\n",
    "    logits = transformer_model(X_train, Y_train, vocab_size, embed_dim, seq_len_encoder, seq_len_decoder, num_heads, num_layers, key)\n",
    "    loss = loss_fn(logits, Y_train)\n",
    "    grads = grad(loss_fn)(logits, Y_train)\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
