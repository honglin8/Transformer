{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from jax import random, grad, jit\n",
    "from jax.nn import softmax, relu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Token Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "# Define a token map with alphanumeric characters and common symbols\n",
    "characters = string.digits + string.punctuation + \" \"\n",
    "token_map = {char: idx for idx, char in enumerate(characters)}\n",
    "vocab_size = len(token_map)\n",
    "\n",
    "def encode_sequence(sequence, token_map):\n",
    "    return [token_map.get(str(token)) for token in sequence]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Embeddings and Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(vocab_size, embed_dim, key):\n",
    "    return random.normal(key, (vocab_size, embed_dim))\n",
    "\n",
    "def get_positional_encoding(seq_len, embed_dim):\n",
    "    pe = np.zeros((seq_len, embed_dim))\n",
    "    for pos in range(seq_len):\n",
    "        for i in range(0, embed_dim, 2):\n",
    "            pe[pos, i] = np.sin(pos / (10000 ** (2 * i / embed_dim)))\n",
    "            pe[pos, i + 1] = np.cos(pos / (10000 ** (2 * i / embed_dim)))\n",
    "    return np.array(pe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(Q, K, V):\n",
    "    d_k = Q.shape[1]\n",
    "    scores = jnp.dot(Q, K.T) / jnp.sqrt(d_k)\n",
    "    weights = softmax(scores, axis=-1)\n",
    "    return jnp.dot(weights, V)\n",
    "\n",
    "def multi_head_attention(Q_split, K_split, V_split, num_heads, W_O):\n",
    "    attention_heads = [scaled_dot_product_attention(Q_split[i], K_split[i], V_split[i]) for i in range(num_heads)]\n",
    "    concat_attention = jnp.concatenate(attention_heads, axis=-1)\n",
    "    return concat_attention @ W_O"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder and Decoder Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_layer(X, num_heads, key, W_O, Q, K, V):\n",
    "    attn_output = multi_head_attention(Q, K, V, num_heads, W_O)\n",
    "    attn_output += X  # Add residual connection\n",
    "    ff_output = relu(attn_output @ random.normal(key, (attn_output.shape[-1], attn_output.shape[-1])))\n",
    "    return ff_output\n",
    "\n",
    "def decoder_layer(Y, encoder_output, num_heads, key, W_O, Q, K, V):\n",
    "    self_attn_output = multi_head_attention(Q, K, V, num_heads, W_O)\n",
    "    cross_attn_output = multi_head_attention(self_attn_output, encoder_output, encoder_output, num_heads, W_O)\n",
    "    cross_attn_output += Y  # Add residual connection\n",
    "    ff_output = relu(cross_attn_output @ random.normal(key, (cross_attn_output.shape[-1], cross_attn_output.shape[-1])))\n",
    "    return ff_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sequence(sequence, token_map):\n",
    "    # Convert sequence to list of token indices\n",
    "    return [token_map.get(str(token)) for token in sequence]\n",
    "\n",
    "def loss_fn(logits, labels):\n",
    "    return -jnp.mean(jnp.sum(labels * jnp.log(logits), axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = random.PRNGKey(0)\n",
    "embed_dim = 16\n",
    "vocab_size = max(token_map.values()) + 1\n",
    "num_heads = 2\n",
    "num_layers = 2\n",
    "\n",
    "X_train_raw = encode_sequence([1,3,2,\"+\",9,4,2], token_map)\n",
    "Y_train_raw = encode_sequence([1,0,7,4], token_map)\n",
    "seq_len_encoder = len(X_train_raw)\n",
    "seq_len_decoder = len(Y_train_raw)\n",
    "\n",
    "W_O = random.normal(random.PRNGKey(0), (embed_dim*num_heads, embed_dim*num_heads))\n",
    "\n",
    "embeddings = get_embeddings(vocab_size, embed_dim, key)\n",
    "\n",
    "encoder_embeddings = embeddings[np.array(X_train_raw)]\n",
    "decoder_embeddings = embeddings[np.array(Y_train_raw)]\n",
    "encoder_positional_encoding = get_positional_encoding(seq_len_encoder, embed_dim)\n",
    "decoder_positional_encoding = get_positional_encoding(seq_len_decoder, embed_dim)\n",
    "\n",
    "# Apply embeddings and positional encodings to encoder and decoder inputs\n",
    "X_embed = encoder_embeddings + encoder_positional_encoding\n",
    "Y_embed = decoder_embeddings + decoder_positional_encoding\n",
    "\n",
    "\n",
    "W_Q_E = {}; W_Q_D = {}; Q_E = {}; Q_D = {}\n",
    "W_K_E = {}; W_K_D = {}; K_E = {}; K_D = {}\n",
    "W_V_E = {}; W_V_D = {}; V_E = {}; V_D = {}\n",
    "\n",
    "for i in range(num_heads):\n",
    "    W_Q_E[i] = random.normal(random.PRNGKey(i), (embed_dim, embed_dim)); W_Q_D[i] = random.normal(random.PRNGKey(i), (embed_dim, embed_dim))\n",
    "    W_K_E[i] = random.normal(random.PRNGKey(i), (embed_dim, embed_dim)); W_K_D[i] = random.normal(random.PRNGKey(i), (embed_dim, embed_dim))\n",
    "    W_V_E[i] = random.normal(random.PRNGKey(i), (embed_dim, embed_dim)); W_V_D[i] = random.normal(random.PRNGKey(i), (embed_dim, embed_dim))\n",
    "    Q_E[i] = X_embed @ W_Q_E[i]; Q_D[i] = X_embed @ W_Q_D[i]\n",
    "    K_E[i] = X_embed @ W_K_E[i]; K_D[i] = X_embed @ W_K_D[i]\n",
    "    V_E[i] = X_embed @ W_V_E[i]; V_D[i] = X_embed @ W_V_D[i]\n",
    "\n",
    "\n",
    "for epoch in range(1000):\n",
    "\n",
    "    # Encoder pass\n",
    "    encoder_output = X_embed\n",
    "    for _ in range(num_layers):\n",
    "        encoder_output = encoder_layer(encoder_output, num_heads, key, W_O, Q_E, K_E, V_E)\n",
    "\n",
    "    # Decoder pass\n",
    "    decoder_output = Y_embed\n",
    "    for _ in range(num_layers):\n",
    "        decoder_output = decoder_layer(decoder_output, encoder_output, num_heads, key, W_O, Q_D, K_D, V_D)\n",
    "    \n",
    "    # Final output layer (logits)\n",
    "    logits = softmax(decoder_output @ encoder_embeddings.T)\n",
    "    \n",
    "    loss = loss_fn(logits, Y_train)\n",
    "    grads = grad(loss_fn)(logits, Y_train)\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': 0,\n",
       " '1': 1,\n",
       " '2': 2,\n",
       " '3': 3,\n",
       " '4': 4,\n",
       " '5': 5,\n",
       " '6': 6,\n",
       " '7': 7,\n",
       " '8': 8,\n",
       " '9': 9,\n",
       " '!': 10,\n",
       " '\"': 11,\n",
       " '#': 12,\n",
       " '$': 13,\n",
       " '%': 14,\n",
       " '&': 15,\n",
       " \"'\": 16,\n",
       " '(': 17,\n",
       " ')': 18,\n",
       " '*': 19,\n",
       " '+': 20,\n",
       " ',': 21,\n",
       " '-': 22,\n",
       " '.': 23,\n",
       " '/': 24,\n",
       " ':': 25,\n",
       " ';': 26,\n",
       " '<': 27,\n",
       " '=': 28,\n",
       " '>': 29,\n",
       " '?': 30,\n",
       " '@': 31,\n",
       " '[': 32,\n",
       " '\\\\': 33,\n",
       " ']': 34,\n",
       " '^': 35,\n",
       " '_': 36,\n",
       " '`': 37,\n",
       " '{': 38,\n",
       " '|': 39,\n",
       " '}': 40,\n",
       " '~': 41,\n",
       " ' ': 42}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_map"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
