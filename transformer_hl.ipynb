{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `SelfAttention`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size // heads\n",
    "\n",
    "        assert (\n",
    "            self.head_dim * self.heads == self.embed_size\n",
    "        ), \"Embedding size needs to be divisible by heads\"\n",
    "\n",
    "        self.values = nn.Linear(embed_size, embed_size)\n",
    "        self.keys = nn.Linear(embed_size, embed_size)\n",
    "        self.queries = nn.Linear(embed_size, embed_size)\n",
    "        self.fc_out = nn.Linear(embed_size, embed_size)\n",
    "\n",
    "    def forward(self, values, keys, query, mask):\n",
    "        N = query.shape[0] # Batch size\n",
    "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
    "        # Shapes before splitting:\n",
    "        # values:  (N, value_len, embed_size)\n",
    "        # keys:    (N, key_len  , embed_size)\n",
    "        # queries: (N, query_len, embed_size)\n",
    "\n",
    "        # Split the embedding into self.heads different pieces\n",
    "        values = self.values(values).reshape(N, value_len, self.heads, self.head_dim)\n",
    "        keys = self.keys(keys).reshape(N, key_len, self.heads, self.head_dim)\n",
    "        queries = self.queries(query).reshape(N, query_len, self.heads, self.head_dim)\n",
    "        # Shapes after splitting: \n",
    "        # values:  (N, value_len, heads, head_dim)\n",
    "        # keys:    (N, key_len  , heads, head_dim)\n",
    "        # queries: (N, query_len, heads, head_dim)\n",
    "\n",
    "        # Compute the dot product between queries and keys for each head, \n",
    "        # and divide by sqrt of head_dim for numerical stability\n",
    "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys]) / math.sqrt(self.head_dim)\n",
    "        # Shape of energy: (N, heads, query_len, key_len)\n",
    "\n",
    "        # Apply mask\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "        # Shape of mask: (N, 1, 1, key_len)\n",
    "        # 0 in key_len dimension means that the respective element in energy is set to -1e20\n",
    "        # Mask will be broadcasted to (N, heads, query_len, key_len) by PyTorch automatically\n",
    "\n",
    "        # Compute the attention weights for each head using the softmax function\n",
    "        attention = torch.softmax(energy, dim=-1)\n",
    "        # Shape of attention: (N, heads, query_len, key_len)\n",
    "\n",
    "        # Multiply the attention weights with the values for each head and then concatenate\n",
    "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
    "            N, query_len, self.embed_size\n",
    "        )\n",
    "        # Shape of out: (N, query_len, embed_size)\n",
    "\n",
    "        out = self.fc_out(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `TransformerBlock`: \n",
    "\n",
    "`SelfAttention` -> layerNorm -> Feed-Forward -> layerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, forward_expansion, dropout):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = SelfAttention(embed_size, heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_size, forward_expansion * embed_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(forward_expansion * embed_size, embed_size),\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, value, key, query, mask):\n",
    "        # Compute self-attention\n",
    "        attention = self.attention(value, key, query, mask)\n",
    "        # Shape of attention: (N, query_len, embed_size)\n",
    "\n",
    "        # Add skip connection, run through normalization and dropout\n",
    "        x = self.dropout(self.norm1(attention + query))\n",
    "        # Shape of x: (N, query_len, embed_size)\n",
    "\n",
    "        # Feed-forward network\n",
    "        forward = self.feed_forward(x)\n",
    "        # Shape of forward: (N, query_len, embed_size)\n",
    "\n",
    "        # Add skip connection, run through normalization and dropout\n",
    "        out = self.dropout(self.norm2(forward + x))\n",
    "        # Shape of out: (N, query_len, embed_size)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder: `num_layers` of `TransformerBlock` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        src_vocab_size, # Size of the source vocabulary\n",
    "        num_layers, # Number of TransformerBlocks\n",
    "        max_length, # Maximum length of the sentence\n",
    "        embed_size,\n",
    "        heads,\n",
    "        forward_expansion,\n",
    "        dropout,\n",
    "        device,\n",
    "    ):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.device = device\n",
    "        self.word_embedding = nn.Embedding(src_vocab_size, embed_size)\n",
    "        self.position_embedding = nn.Embedding(max_length, embed_size)\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                TransformerBlock(\n",
    "                    embed_size,\n",
    "                    heads,\n",
    "                    forward_expansion,\n",
    "                    dropout,\n",
    "                )\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        N, seq_length = x.shape\n",
    "\n",
    "        # Positions is the index of the word in the sentence (0, 1, 2, ..., seq_length)\n",
    "        positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)\n",
    "\n",
    "        # Add word embeddings and position embeddings\n",
    "        out = self.dropout(\n",
    "            (self.word_embedding(x) + self.position_embedding(positions))\n",
    "        )\n",
    "        # Shape of out: (N, seq_length, embed_size)\n",
    "\n",
    "        # In the Encoder the query, key, value are all the same\n",
    "        for layer in self.layers:\n",
    "            out = layer(out, out, out, mask)\n",
    "            # Shape of out: (N, seq_length, embed_size)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `DecoderBlock`\n",
    "\n",
    "Masked `SelfAttention` -> layerNorm -> `TransformerBlock`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, forward_expansion, dropout):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.norm = nn.LayerNorm(embed_size)\n",
    "        self.masked_attention = SelfAttention(embed_size, heads=heads)\n",
    "        self.transformer_block = TransformerBlock(\n",
    "            embed_size, heads, forward_expansion, dropout\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, encoder_value, encoder_key, src_mask, trg_mask):\n",
    "        # Self attention on the target sentence with mask\n",
    "        attention = self.masked_attention(x, x, x, trg_mask)\n",
    "\n",
    "        # Add skip connection, run through normalization and dropout\n",
    "        query = self.dropout(self.norm(attention + x))\n",
    "\n",
    "        # Transformer block with encoder's output as value and key\n",
    "        out = self.transformer_block(encoder_value, encoder_key, query, src_mask)\n",
    "        # Shape of out: (N, query_len, embed_size)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder: `num_layers` of `DecoderBlock`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        trg_vocab_size, # Size of the target vocabulary\n",
    "        num_layers, # Number of DecoderBlocks\n",
    "        max_length, # Maximum length of the sentence\n",
    "        embed_size,\n",
    "        heads,\n",
    "        forward_expansion,\n",
    "        dropout,\n",
    "        device,\n",
    "    ):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.device = device\n",
    "        self.word_embedding = nn.Embedding(trg_vocab_size, embed_size)\n",
    "        self.position_embedding = nn.Embedding(max_length, embed_size)\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                DecoderBlock(\n",
    "                    embed_size,\n",
    "                    heads,\n",
    "                    forward_expansion,\n",
    "                    dropout)\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.fc_out = nn.Linear(embed_size, trg_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, enc_out, src_mask, trg_mask):\n",
    "        N, seq_length = x.shape\n",
    "\n",
    "        # Positions is the index of the word in the sentence (0, 1, 2, ..., seq_length)\n",
    "        positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)\n",
    "\n",
    "        # Add word embeddings and position embeddings\n",
    "        x = self.dropout((self.word_embedding(x) + self.position_embedding(positions)))\n",
    "\n",
    "        # In the Decoder the key and value are the encoder's output,\n",
    "        # and the query is the output of the previous DecoderBlock\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, enc_out, enc_out, src_mask, trg_mask)\n",
    "\n",
    "        out = self.fc_out(x)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Transformer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        src_vocab_size,\n",
    "        trg_vocab_size,\n",
    "        src_pad_idx, # Index of the padding token in the source vocabulary\n",
    "        trg_pad_idx, # Index of the padding token in the target vocabulary\n",
    "        num_layers,\n",
    "        max_length,\n",
    "        embed_size,\n",
    "        heads,\n",
    "        forward_expansion=4,\n",
    "        dropout=0.0,\n",
    "        device=\"cpu\",\n",
    "    ):\n",
    "\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        # Initialize the Encoder\n",
    "        self.encoder = Encoder(\n",
    "            src_vocab_size, # Size of the source vocabulary\n",
    "            num_layers, # Number of TransformerBlocks\n",
    "            max_length, # Maximum length of the sentence\n",
    "            embed_size,\n",
    "            heads,\n",
    "            forward_expansion,\n",
    "            dropout,\n",
    "            device,\n",
    "        )\n",
    "\n",
    "        # Initialize the Decoder\n",
    "        self.decoder = Decoder(\n",
    "            trg_vocab_size, # Size of the target vocabulary\n",
    "            num_layers, # Number of DecoderBlocks\n",
    "            max_length, # Maximum length of the sentence\n",
    "            embed_size,\n",
    "            heads,\n",
    "            forward_expansion,\n",
    "            dropout,\n",
    "            device,\n",
    "        )\n",
    "\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.device = device\n",
    "\n",
    "    def make_src_mask(self, src):\n",
    "        # Shape of src: (N, src_len)\n",
    "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        # Shape of src_mask: (N, 1, 1, src_len)\n",
    "        return src_mask.to(self.device)\n",
    "\n",
    "    def make_trg_mask(self, trg):\n",
    "        N, trg_len = trg.shape\n",
    "\n",
    "        # Create a lower triangular matrix of ones with shape (trg_len, trg_len),\n",
    "        # then expand it to (N, 1, trg_len, trg_len)\n",
    "        trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(\n",
    "            N, 1, trg_len, trg_len\n",
    "        )\n",
    "\n",
    "        return trg_mask.to(self.device)\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        src_mask = self.make_src_mask(src)\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "        enc_src = self.encoder(src, src_mask)\n",
    "        out = self.decoder(trg, enc_src, src_mask, trg_mask)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the `Transformer`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "num_samples = 5000 # Number of samples in the dataset\n",
    "max_length = 8 # Maximum length of the sequence including the <SOS> and <EOS> tokens\n",
    "vocab_size = 99 + 3 # Numbers from 1 to 99 and three indices for padding, start of sequence, and end of sequence\n",
    "sos_idx = 100 # Start of sequence index\n",
    "eos_idx = 101 # End of sequence index\n",
    "pad_idx = 0 # Padding index\n",
    "num_layers = 2 # Number of Blocks in the Encoder and Decoder\n",
    "embed_size = 32 # Embedding size for the tokens\n",
    "heads = 2 # Number of heads in the Multi-Head Attention\n",
    "forward_expansion = 4\n",
    "dropout = 0.0\n",
    "learning_rate = 0.001\n",
    "batch_size = 256\n",
    "num_epochs = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copy Task - Source Data:\n",
      "tensor([[100,  52,   7,  58,  78,  78, 101,   0],\n",
      "        [100,  16,  55, 101,   0,   0,   0,   0],\n",
      "        [100,  24,  35, 101,   0,   0,   0,   0]])\n",
      "Copy Task - Target Data:\n",
      "tensor([[100,  52,   7,  58,  78,  78, 101,   0],\n",
      "        [100,  16,  55, 101,   0,   0,   0,   0],\n",
      "        [100,  24,  35, 101,   0,   0,   0,   0]])\n",
      "\n",
      "Reverse Task - Source Data:\n",
      "tensor([[100,  98,  10,  20,  97,  16,  91, 101],\n",
      "        [100,  21,  11,  88, 101,   0,   0,   0],\n",
      "        [100,  43,  15, 101,   0,   0,   0,   0]])\n",
      "Reverse Task - Target Data:\n",
      "tensor([[100,  91,  16,  97,  20,  10,  98, 101],\n",
      "        [100,  88,  11,  21, 101,   0,   0,   0],\n",
      "        [100,  15,  43, 101,   0,   0,   0,   0]])\n",
      "\n",
      "Sort Task - Source Data:\n",
      "tensor([[100,  87,  90,  37,  85,  98,  51, 101],\n",
      "        [100,  74,  74, 101,   0,   0,   0,   0],\n",
      "        [100,  38,  15,  11,  64, 101,   0,   0]])\n",
      "Sort Task - Target Data:\n",
      "tensor([[100,  37,  51,  85,  87,  90,  98, 101],\n",
      "        [100,  74,  74, 101,   0,   0,   0,   0],\n",
      "        [100,  11,  15,  38,  64, 101,   0,   0]])\n"
     ]
    }
   ],
   "source": [
    "# Function to generate data for the three tasks: copy, reverse, and sort\n",
    "def generate_data(num_samples, max_length, pad_idx, sos_idx, eos_idx, task):\n",
    "    src_data = []\n",
    "    trg_data = []\n",
    "    \n",
    "    for _ in range(num_samples):\n",
    "        seq_length = np.random.randint(1, max_length - 1)  # Length of the random sequence\n",
    "        sequence = torch.randint(1, 100, (seq_length,))  # Numbers from 1 to 99\n",
    "        \n",
    "        # Create the source sequence with <SOS> at the start and <EOS> at the end\n",
    "        src_sequence = torch.cat([torch.tensor([sos_idx]), sequence, torch.tensor([eos_idx])])\n",
    "        \n",
    "        # Pad the source sequence to the maximum length\n",
    "        padded_src_sequence = torch.cat([src_sequence, torch.full((max_length - len(src_sequence),), pad_idx)])\n",
    "        \n",
    "        if task == 'copy':\n",
    "            # For copying task, target is the same as source\n",
    "            padded_trg_sequence = padded_src_sequence.clone()\n",
    "        \n",
    "        elif task == 'reverse':\n",
    "            # Reverse the sequence for the target and pad it to the maximum length\n",
    "            reversed_sequence = sequence.flip(0)\n",
    "            trg_sequence = torch.cat([torch.tensor([sos_idx]), reversed_sequence, torch.tensor([eos_idx])])\n",
    "            padded_trg_sequence = torch.cat([trg_sequence, torch.full((max_length - len(trg_sequence),), pad_idx)])\n",
    "        \n",
    "        elif task == 'sort':\n",
    "            # Sort the sequence for the target and pad it to the maximum length\n",
    "            sorted_sequence = sequence.sort().values\n",
    "            trg_sequence = torch.cat([torch.tensor([sos_idx]), sorted_sequence, torch.tensor([eos_idx])])\n",
    "            padded_trg_sequence = torch.cat([trg_sequence, torch.full((max_length - len(trg_sequence),), pad_idx)])\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(\"Invalid task. Choose from 'copy', 'reverse', or 'sort'.\")\n",
    "        \n",
    "        src_data.append(padded_src_sequence)\n",
    "        trg_data.append(padded_trg_sequence)\n",
    "    \n",
    "    src_data = torch.stack(src_data)\n",
    "    trg_data = torch.stack(trg_data)\n",
    "    \n",
    "    return src_data, trg_data\n",
    "\n",
    "# Generate data for the copying task\n",
    "src_data_copy, trg_data_copy = generate_data(3, max_length, pad_idx, sos_idx, eos_idx, task='copy')\n",
    "\n",
    "# Generate data for the reversing task\n",
    "src_data_reverse, trg_data_reverse = generate_data(3, max_length, pad_idx, sos_idx, eos_idx, task='reverse')\n",
    "\n",
    "# Generate data for the sorting task\n",
    "src_data_sort, trg_data_sort = generate_data(3, max_length, pad_idx, sos_idx, eos_idx, task='sort')\n",
    "\n",
    "print(\"Copy Task - Source Data:\")\n",
    "print(src_data_copy)\n",
    "print(\"Copy Task - Target Data:\")\n",
    "print(trg_data_copy)\n",
    "print()\n",
    "print(\"Reverse Task - Source Data:\")\n",
    "print(src_data_reverse)\n",
    "print(\"Reverse Task - Target Data:\")\n",
    "print(trg_data_reverse)\n",
    "print()\n",
    "print(\"Sort Task - Source Data:\")\n",
    "print(src_data_sort)\n",
    "print(\"Sort Task - Target Data:\")\n",
    "print(trg_data_sort)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 4.272413802146912\n",
      "Epoch [2/20], Loss: 4.081902801990509\n",
      "Epoch [3/20], Loss: 3.96914746761322\n",
      "Epoch [4/20], Loss: 3.771239864826202\n",
      "Epoch [5/20], Loss: 3.5348363876342774\n",
      "Epoch [6/20], Loss: 3.2725308179855346\n",
      "Epoch [7/20], Loss: 2.8912335872650146\n",
      "Epoch [8/20], Loss: 2.313313138484955\n",
      "Epoch [9/20], Loss: 1.64979829788208\n",
      "Epoch [10/20], Loss: 1.0643664211034776\n",
      "Epoch [11/20], Loss: 0.6409927934408188\n",
      "Epoch [12/20], Loss: 0.3919969603419304\n",
      "Epoch [13/20], Loss: 0.25072157457470895\n",
      "Epoch [14/20], Loss: 0.1724546119570732\n",
      "Epoch [15/20], Loss: 0.12661609649658204\n",
      "Epoch [16/20], Loss: 0.0977422546595335\n",
      "Epoch [17/20], Loss: 0.07838640697300434\n",
      "Epoch [18/20], Loss: 0.06462651155889035\n",
      "Epoch [19/20], Loss: 0.05438430868089199\n",
      "Epoch [20/20], Loss: 0.04661877769976854\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "# Generate data for the given task\n",
    "task = 'reverse'\n",
    "src_data, trg_data = generate_data(num_samples, max_length, pad_idx, sos_idx, eos_idx, task)\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataset = TensorDataset(src_data, trg_data)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize the Transformer model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Transformer(\n",
    "    src_vocab_size=vocab_size,\n",
    "    trg_vocab_size=vocab_size,\n",
    "    src_pad_idx=pad_idx,\n",
    "    trg_pad_idx=pad_idx,\n",
    "    num_layers=num_layers,\n",
    "    max_length=max_length,\n",
    "    embed_size=embed_size,\n",
    "    heads=heads,\n",
    "    forward_expansion=forward_expansion,\n",
    "    dropout=dropout,\n",
    "    device=device\n",
    ").to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for src, trg in train_loader:\n",
    "        src = src.to(device)\n",
    "        trg = trg.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(src, trg[:, :-1])\n",
    "        output = output.reshape(-1, output.shape[2])\n",
    "        trg = trg[:, 1:].reshape(-1)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(output, trg)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss/len(train_loader)}')\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Tensor: tensor([[100,   2,   7,   8,   1, 101,   0,   0]])\n"
     ]
    }
   ],
   "source": [
    "# Example input sequence\n",
    "input_sequence = [2,7,8,1]\n",
    "\n",
    "# Prepare the input sequence: Add <SOS> and <EOS> tokens, pad the sequence to the maximum length\n",
    "input_tensor = torch.tensor([sos_idx] + input_sequence + [eos_idx] + [pad_idx] * (max_length - len(input_sequence) - 2)).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "# Move the input tensor to the appropriate device\n",
    "input_tensor = input_tensor.to(device)\n",
    "print(\"Input Tensor:\", input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Tensor: tensor([[100,   1,   8,   7,   2, 101,   0,   0]])\n"
     ]
    }
   ],
   "source": [
    "# Function to generate output from the model\n",
    "def generate_output(model, input_tensor, max_length, pad_idx, sos_idx, eos_idx):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Create a target tensor filled with pad_idx\n",
    "        trg_tensor = torch.full((1, max_length), pad_idx).to(device)\n",
    "        \n",
    "        # Set the first token of the target tensor to the start token\n",
    "        trg_tensor[0, 0] = sos_idx\n",
    "        \n",
    "        for i in range(1, max_length):\n",
    "            # Pass the input and target tensors through the model\n",
    "            output = model(input_tensor, trg_tensor[:, :i])\n",
    "            \n",
    "            # Get the token with the highest probability\n",
    "            next_token = output.argmax(2)[:, -1]\n",
    "            \n",
    "            # Set the next token in the target tensor\n",
    "            trg_tensor[0, i] = next_token.item()\n",
    "            \n",
    "            # Stop if the next token is the end token\n",
    "            if next_token.item() == eos_idx:\n",
    "                break\n",
    "        \n",
    "        return trg_tensor\n",
    "\n",
    "# Generate output\n",
    "output_tensor = generate_output(model, input_tensor, max_length, pad_idx, sos_idx, eos_idx)\n",
    "\n",
    "print(\"Output Tensor:\", output_tensor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
